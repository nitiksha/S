package pack

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions._

object obj {
  def main(args:Array[String]):Unit={
  println("===Hello====")

   val conf = new SparkConf().setAppName("first").setMaster("local[*]").set("spark.driver.host","localhost")
   .set("spark.driver.allowMultipleContexts", "true")

   val sc = new SparkContext(conf)

   sc.setLogLevel("ERROR")

   val spark = SparkSession.builder.getOrCreate()

   import spark.implicits._


   val df = Seq(
     ("m1", "m1,m2","m1,m2,m3","m1,m2,m3,m4")
     ).toDF("col1","col2","col3","col4")    


   df.show()  

   
   val  concat = df.selectExpr("concat(col1,'~',col2,'~',col3,'~',col4) as col")
   
   
   concat.show(false)
  
   

   val explodesplit = concat.selectExpr("explode(split(col,'~')) as col")
   
   
   explodesplit.show()
    
  }
}
