package pack

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions._
import org.apache.spark.rdd.RDD

object obj {
  
  case class schema(first_name:String,	last_name:String,	company_name:String,		address:String
)
  
  def main(args:Array[String]):Unit={
    
			println("===Hello====")

			val conf = new SparkConf().setAppName("first").setMaster("local[*]").set("spark.driver.host","localhost")
			.set("spark.driver.allowMultipleContexts", "true")

			val sc = new SparkContext(conf)

			sc.setLogLevel("ERROR")

			val spark = SparkSession.builder.getOrCreate()
			
	import spark.implicits._
	
          val dsldf = Seq(
                        ("00000000", "06-26-2011", 200, "Exercise", "GymnasticsPro", "cash"),
                        ("00000001", "05-26-2011", 300, "Exercise", "Weightlifting", "credit"),
                        ("00000002", "06-01-2011", 100, "Exercise", "GymnasticsPro", "cash"),
                        ("00000003", "06-05-2011", 100, "Gymnastics", "Rings", "credit"),
                        ("00000004", "12-17-2011", 300, "Team Sports", "Field", "paytm"),
                        ("00000005", "02-14-2011", 200, "Gymnastics", null, "cash")
                        ).toDF("txn","txndate","amount","category","product","spendby")
                        
                        dsldf.show()
                        
                        dsldf.printSchema()
                        
              val fildf = dsldf.selectExpr("upper(product) as pro")
              fildf.show()
  }
  
  
}
