package pack

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions._

object obj {
  def main(args:Array[String]):Unit={
    println("jai ganesha")
    
      val conf = new SparkConf().setAppName("first").setMaster("local[*]").set("spark.driver.host","localhost").set("spark.driver.allowMultipleContexts", "true")

       val sc = new SparkContext(conf)

       sc.setLogLevel("ERROR")

      val spark = SparkSession.builder.getOrCreate()

      import spark.implicits._

    
    val df1 = Seq((1,"id","1000"),
                  (1,"name","john"),
                   (2,"id","2000"),
                  (2,"name","fred"),
                   (3,"id","3000"),
                   (3,"id","4000"),
                  (3,"name","sunny")
                   ).toDF("emp_id","key","values").orderBy("emp_id")
                   
                   df1.show()
    
val df2 = df1.groupBy("emp_id").pivot("key").agg(first("values"))

   df2.show()//pivot
    
    
  }
}
